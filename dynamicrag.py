# -*- coding: utf-8 -*-
"""DynamicRag.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pd3ReO1vNzUrp0wYLMmxrK2X-t2ddgDB
"""

!pip install "unstructured[all-docs]"

!apt-get install -y poppler-utils

import requests

global file_path

print("Choose \n 1 if file is uploaded \n 2 for download link  ")
print("\n")
choice = input("Your choice:")
print("\n")

if choice == '1':
    file_path = input("Enter the name of the uploaded file: ")
    print(f"You chose to use the uploaded file: {file_path}")



elif choice == '2':
    URL = input("Enter the download link of the pdf: ")
    download_filename = "downloaded_file"
    response = requests.get(URL)
    if response.status_code== 200 :
      with open(download_filename,"wb") as file:
        file.write(response.content)
      file_path = download_filename

      print(f"The file has been downloaded and saved as {download_filename}")

    else:
      print(f"Failed to Download the file. Status Code: {response.status_code}")
      file_path = None


else:
    print("Invalid choice. Please run the cell again and enter 1 or 2.")

from unstructured.partition.pdf import partition_pdf


chunks = partition_pdf(
      filename=file_path,
      languages=["eng", "urd","ind","msa","fra","spa","deu"],
      infer_table_structure=True,            # extract tables
      strategy="hi_res",                     # mandatory to infer tables

      extract_image_block_types=[],   # Set to empty list to not extract images

      extract_image_block_to_payload=True,   # if true, will extract base64 for API usage

      chunking_strategy="by_title",          # or 'basic'
      max_characters=7000,                  # defaults to 500
      combine_text_under_n_chars=1500,       # defaults to 0
      new_after_n_chars=7000,
      overlap=50, # Add this line to specify overlap
      clean_extra_whitespace=True # Add this line to clean extra whitespace
  )


chunks = [chunk for chunk in chunks if chunk.text]

set([str(type(el)) for el in chunks])

print(f"Number of chunks: {len(chunks)}")
if len(chunks) > 0:

  print("Inspecting the first chunk:")
  display(chunks[0].metadata.orig_elements)
else:
  print("The 'chunks' list is empty.")

all_elements = []

for chunk in chunks:
    if hasattr(chunk, 'metadata') and hasattr(chunk.metadata, 'orig_elements'):
        all_elements.extend(chunk.metadata.orig_elements)

chunk_images = [el for el in all_elements if 'Image' in str(type(el))]

if chunk_images:
    display(chunk_images[0].to_dict())
else:
    print("No image elements found in the chunks.")

len(chunks)

tables = []
texts = []

for chunk in chunks:
    if "Table" in str(type(chunk)):
        tables.append(chunk)

    if "CompositeElement" in str(type((chunk))):
        texts.append(chunk)

def get_images_base64(chunks):
    images_b64 = []
    for chunk in chunks:
        if "CompositeElement" in str(type(chunk)):
            chunk_els = chunk.metadata.orig_elements
            for el in chunk_els:
                if "Image" in str(type(el)) and hasattr(el.metadata, 'image_base64') and el.metadata.image_base64 is not None:
                    images_b64.append(el.metadata.image_base64)
    return images_b64

images = get_images_base64(chunks)

for i, chunk in enumerate(chunks):
  print(f"Chunk {i+1}:\n{chunk.text}\n---")

import base64
from IPython.display import Image, display

def display_base64_image(base64_code):

    image_data = base64.b64decode(base64_code)

    display(Image(data=image_data))

if images:
    display_base64_image(images[0])
else:
    print("No images to display.")

if tables:
  print("Displaying Tables:")
  for i, table in enumerate(tables):
      print(f"Table {i+1}:\n{table.text}\n---")
else:
  print("No tables were extracted from the PDF.")

!pip install -U torch # requires torch 2.1.1+ (for efficient sdpa implementation)
!pip install tqdm # for progress bars
!pip install sentence-transformers # for embedding models

from sentence_transformers import SentenceTransformer

embedding_model = SentenceTransformer("Alibaba-NLP/gte-multilingual-base", trust_remote_code=True)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# from tqdm.auto import tqdm
# import numpy as np
# 
# chunk_texts = [chunk.text for chunk in chunks]
# 
# chunk_embeddings = []
# for text in tqdm(chunk_texts, desc="Generating Embeddings"):
# 
#     embedding = embedding_model.encode(text, show_progress_bar=False)
#     chunk_embeddings.append(embedding)
# 
# 
# chunk_embeddings = np.array(chunk_embeddings)
# 
# print(f"Shape of chunk embeddings: {chunk_embeddings.shape}")

display(chunk_embeddings)

chunk_embeddings.shape

import torch
from sentence_transformers import util
import numpy as np # Import numpy

query="Single entries from other screens"
print(f"Query: {query}")

# embedding_model_name = "Alibaba-NLP/gte-multilingual-base" # Removed unnecessary variable
query_embedding_np = embedding_model.encode(str(query)) # Ensure query is a standard string
query_embedding = torch.tensor(query_embedding_np, dtype=torch.float32).to("cpu") # Convert numpy array to torch tensor

from time import perf_counter as timer

start_time = timer()
embeddings_tensor = torch.tensor(chunk_embeddings, dtype=torch.float32).to("cpu")

cos_sim  = util.cos_sim(a=query_embedding, b= embeddings_tensor)[0]
end_time= timer()

print(f"[INFO] Time taken to get scores on {len(embeddings_tensor)} embeddings: {end_time - start_time: .5f} seconds.")

k = min(5, len(embeddings_tensor))
if k > 0:
    top_results_cos_sim = torch.topk(cos_sim, k=k)
    print(top_results_cos_sim)
else:
    print("No embeddings available to get top results from.")

# Commented out IPython magic to ensure Python compatibility.
# %pip install -U sentence-transformers

from sentence_transformers import CrossEncoder

reranker = CrossEncoder('Alibaba-NLP/gte-multilingual-reranker-base', trust_remote_code=True)

top_result_indices = top_results_cos_sim.indices.tolist()

top_chunks_text = [chunks[i].text for i in top_result_indices]

rerank_pairs = [[query, chunk_text] for chunk_text in top_chunks_text]

rerank_scores = reranker.predict(rerank_pairs)

reranked_sorted_indices = [idx for _, idx in sorted(zip(rerank_scores, top_result_indices), key=lambda pair: pair[0], reverse=True)]
reranked_sorted_scores = sorted(rerank_scores, reverse=True)


print("Reranked Top 5 Results using Alibaba-NLP/gte-multilingual-reranker-base:")
for i, idx in enumerate(reranked_sorted_indices):
    print(f"Rank {i+1}: Chunk {idx+1} (Score: {reranked_sorted_scores[i]:.4f})")
    print(f"Content: {chunks[idx].text[:200]}...")
    print("---")

# Commented out IPython magic to ensure Python compatibility.
# %pip install PyMuPDF

import fitz
import matplotlib.pyplot as plt
import numpy as np

top_page_numbers = sorted(list(set([chunks[idx].metadata.page_number for idx in reranked_sorted_indices])))

print(f"Displaying images from the top ranked pages: {top_page_numbers}")

pdf_path = file_path

doc = fitz.open(pdf_path)

for page_num in top_page_numbers:

    page = doc.load_page(page_num - 1)


    img = page.get_pixmap(dpi=150)
    if img.n == 1:
        img_array = np.frombuffer(img.samples_mv, dtype=np.uint8).reshape((img.h, img.w))
        cmap = 'gray'
    else:
        img_array = np.frombuffer(img.samples_mv, dtype=np.uint8).reshape((img.h, img.w, img.n))
        cmap = None


    plt.figure(figsize=(10, 8))
    plt.imshow(img_array, cmap=cmap)
    plt.title(f"Image from Page {page_num} (Top Ranked)")
    plt.axis('off')
    plt.show()


doc.close()

import fitz
import matplotlib.pyplot as plt
import numpy as np
import torch
from sentence_transformers import util

def retrieve_and_display_images(query, model, reranker, chunks, pdf_path, top_k=5):

    print(f"Processing query: {query}")


    query_embedding = model.encode(query, convert_to_tensor=True).to("cpu").to(torch.float32)


    if 'chunk_embeddings' not in globals():
        print("Error: chunk_embeddings not found. Please run the embedding cell first.")
        return

    embeddings_tensor = torch.tensor(chunk_embeddings, dtype=torch.float32).to("cpu")



    cos_sim = util.cos_sim(a=query_embedding, b=embeddings_tensor)[0]


    top_results_cos_sim = torch.topk(cos_sim, k=top_k)
    top_result_indices = top_results_cos_sim.indices.tolist()
    top_chunks_text = [chunks[i].text for i in top_result_indices]


    rerank_pairs = [[query, chunk_text] for chunk_text in top_chunks_text]
    rerank_scores = reranker.predict(rerank_pairs)


    reranked_sorted_indices = [idx for _, idx in sorted(zip(rerank_scores, top_result_indices), key=lambda pair: pair[0], reverse=True)]

    print("\nReranked Top Results:")
    for i, idx in enumerate(reranked_sorted_indices):
        print(f"Rank {i+1}: Chunk {idx+1} (Score: {rerank_scores[i]:.4f})")
        print(f"Content: {chunks[idx].text[:100]}...")
        print(f"Page: {chunks[idx].metadata.page_number}")
        print("---")


    top_page_numbers = sorted(list(set([chunks[idx].metadata.page_number for idx in reranked_sorted_indices])))

    print(f"\nDisplaying images from the top ranked pages: {top_page_numbers}")

    try:
        doc = fitz.open(pdf_path)
        for page_num in top_page_numbers:
            try:

                page = doc.load_page(page_num - 1)


                img = page.get_pixmap(dpi=150)


                if img.n == 1:
                    img_array = np.frombuffer(img.samples_mv, dtype=np.uint8).reshape((img.h, img.w))
                    cmap = 'gray'
                else:
                    img_array = np.frombuffer(img.samples_mv, dtype=np.uint8).reshape((img.h, img.w, img.n))
                    cmap = None

                plt.figure(figsize=(10, 8))
                plt.imshow(img_array, cmap=cmap)
                plt.title(f"Image from Page {page_num} (Top Ranked)")
                plt.axis('off')
                plt.show()
            except Exception as e:
                print(f"Error displaying image from page {page_num}: {e}")
        doc.close()
    except FileNotFoundError:
        print(f"Error: PDF file not found at {pdf_path}")
    except Exception as e:
        print(f"An error occurred while processing the PDF: {e}")

import google.generativeai as genai
from google.colab import userdata

# Get the API key from Colab's secrets manager
GOOGLE_API_KEY = userdata.get('python')

# Configure the genai library with the API key
genai.configure(api_key=GOOGLE_API_KEY)

# Now you can use the models directly, without explicitly creating a client instance
model = genai.GenerativeModel('gemini-2.5-flash')

response = model.generate_content(
    contents="Explain how AI works in a few words",
)

print(response.text)

if 'query' not in globals():
    query = "What is the process for creating a sales invoice?"
    print(f"Using sample query: {query}")
else:
    print(f"Using existing query: {query}")


context_chunks = []
if 'reranked_sorted_indices' in globals():
    context_chunks = [chunks[idx] for idx in reranked_sorted_indices]
    print(f"Using {len(context_chunks)} reranked top chunks for context.")
elif 'top_result_indices' in globals():
     context_chunks = [chunks[idx] for idx in top_result_indices]
     print(f"Using {len(context_chunks)} initial top chunks for context (reranking not used or variable not found).")
else:
     print("Error: Top result indices not found. Please ensure retrieval/reranking steps were run successfully.")


if context_chunks:

    context_text = "\n---\n".join([f"Chunk from Page {chunk.metadata.page_number}:\n{chunk.text}" for chunk in context_chunks])


    prompt = f"""You are an AI assistant that answers questions based on the provided context.
Read the following context carefully and answer the question below.
If the answer cannot be found in the context, state that you cannot answer based on the provided information.
Keep your answer concise and directly address the question.

Context:
{context_text}

Question:
{query}

Answer:
"""
    print("\nFormatted Prompt (first 500 characters):")
    print(prompt[:500] + "...")
else:
    prompt = "Could not generate prompt due to missing context chunks."
    print(prompt)

try:
    gemini_model = genai.GenerativeModel('gemini-2.5-flash')


    response = gemini_model.generate_content(prompt)


    print("\nGenerated Answer:")
    print(response.text)

except Exception as e:
    print(f"Error generating response from Gemini API: {e}")
    print("Please ensure you have run the cell to initialize the Gemini client (Step 3).")

import fitz
import matplotlib.pyplot as plt
import numpy as np
import torch
from sentence_transformers import util
import google.generativeai as genai
from google.colab import userdata
import textwrap # Import textwrap
from sentence_transformers import SentenceTransformer # Import SentenceTransformer

try:
    google_api_key = userdata.get('GOOGLE_API_KEY')
    if google_api_key:
        genai.configure(api_key=google_api_key)

    else:
        print("GOOGLE_API_KEY not found. Gemini client not configured for pipeline.")
except Exception as e:
    print(f"Error configuring Gemini client for pipeline: {e}")




def rag_pipeline(query, embedding_model_obj, chunks, chunk_embeddings, pdf_path, top_k=5, use_reranker=True, reranker=None, max_output_tokens=1024, display_images=True):


    print(f"Processing query through RAG pipeline: {query}")

    query_embedding = embedding_model_obj.encode(query, convert_to_tensor=True).to("cpu").to(torch.float32)


    embeddings_tensor = torch.tensor(chunk_embeddings, dtype=torch.float32).to("cpu")


    cos_sim = util.cos_sim(a=query_embedding, b=embeddings_tensor)[0]

    # Ensure k is not greater than the number of available chunks
    k = min(top_k, len(chunks))
    if k == 0:
        print("No chunks available to retrieve results from.")
        return "No relevant context found to answer the query."


    top_results_cos_sim = torch.topk(cos_sim, k=k)
    top_result_indices = top_results_cos_sim.indices.tolist()


    if use_reranker:
        if reranker is None:
            print("Warning: Reranker is not provided but use_reranker is True. Skipping reranking.")
            context_indices = top_result_indices
        else:
            print(f"Reranking top {k} results.")
            top_chunks_text = [chunks[i].text for i in top_result_indices]
            rerank_pairs = [[query, chunk_text] for chunk_text in top_chunks_text]
            rerank_scores = reranker.predict(rerank_pairs)
            context_indices = [idx for _, idx in sorted(zip(rerank_scores, top_result_indices), key=lambda pair: pair[0], reverse=True)]
    else:
        context_indices = top_result_indices


    context_chunks_for_llm = [chunks[idx] for idx in context_indices]

    if not context_chunks_for_llm:

        if display_images:

             top_page_numbers = sorted(list(set([chunks[idx].metadata.page_number for idx in top_result_indices])))
             print(f"\nNo relevant text context found, but displaying images from top initial result pages: {top_page_numbers}")

             try:
                 doc = fitz.open(pdf_path)
                 for page_num in top_page_numbers:
                     try:
                         page = doc.load_page(page_num - 1)
                         img = page.get_pixmap(dpi=150)
                         if img.n == 1:
                             img_array = np.frombuffer(img.samples_mv, dtype=np.uint8).reshape((img.h, img.w))
                             cmap = 'gray'
                         else:
                             img_array = np.frombuffer(img.samples_mv, dtype=np.uint8).reshape((img.h, img.w, img.n))
                             cmap = None
                         plt.figure(figsize=(10, 8))
                         plt.imshow(img_array, cmap=cmap)
                         plt.title(f"Image from Page {page_num} (Top Result)")
                         plt.axis('off')
                         plt.show()
                     except Exception as e:
                         print(f"Error displaying image from page {page_num}: {e}")
                 doc.close()
             except FileNotFoundError:
                 print(f"Error: PDF file not found at {pdf_path}")
             except Exception as e:
                 print(f"An error occurred while processing the PDF for image display: {e}")
        return "No relevant context found to answer the query."

    context_text = "\n---\n".join([f"Chunk from Page {chunk.metadata.page_number}:\n{chunk.text}" for chunk in context_chunks_for_llm])



    if display_images:

        context_page_numbers = sorted(list(set([chunk.metadata.page_number for chunk in context_chunks_for_llm])))
        print(f"\nDisplaying images from the context pages: {context_page_numbers}")

        try:
            doc = fitz.open(pdf_path)
            for page_num in context_page_numbers:
                try:

                    page = doc.load_page(page_num - 1)


                    img = page.get_pixmap(dpi=150)


                    if img.n == 1:
                        img_array = np.frombuffer(img.samples_mv, dtype=np.uint8).reshape((img.h, img.w))
                        cmap = 'gray'
                    else:
                        img_array = np.frombuffer(img.samples_mv, dtype=np.uint8).reshape((img.h, img.w, img.n))
                        cmap = None

                    plt.figure(figsize=(10, 8))
                    plt.imshow(img_array, cmap=cmap)
                    plt.title(f"Image from Page {page_num} (Context)")
                    plt.axis('off')
                    plt.show()
                except Exception as e:
                    print(f"Error displaying image from page {page_num}: {e}")
            doc.close()
        except FileNotFoundError:
            print(f"Error: PDF file not found at {pdf_path}")
        except Exception as e:
            print(f"An error occurred while processing the PDF for image display: {e}")



    prompt = f"""You are an AI assistant that answers questions based on the provided context.
Read the following context carefully and provide a detailed answer to the question below.
If the answer cannot be found in the context, state that you cannot answer based on the provided information.

Context:
{context_text}

Question:
{query}

Detailed Answer:
"""

    try:

        if 'gemini_model' not in globals() or not isinstance(gemini_model, genai.GenerativeModel):
             llm_model = genai.GenerativeModel('gemini-2.5-flash')
        else:
             llm_model = gemini_model


        response = llm_model.generate_content(prompt, generation_config=genai.GenerationConfig(max_output_tokens=max_output_tokens))
        return response.text
    except Exception as e:
        return f"Error generating response from LLM: {e}"

import textwrap

query = "what is the poem about"
generated_answer = rag_pipeline(query, embedding_model, chunks, chunk_embeddings, file_path, top_k=5, use_reranker=True, reranker=reranker, max_output_tokens=7000, display_images=True)

wrapped_answer = textwrap.fill(generated_answer, width=80)
print(wrapped_answer)